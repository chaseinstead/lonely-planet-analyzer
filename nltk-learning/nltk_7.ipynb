{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Learning to Classify Text\n",
    "Detecting patterns is a central part of Natural Language Processing. Words ending in -ed tend to be past tense verbs (5.). Frequent use of will is indicative of news text (3). These observable patterns — word structure and word frequency — happen to correlate with particular aspects of meaning, such as tense and topic. But how did we know where to start looking, which aspects of form to associate with which aspects of meaning?\n",
    "\n",
    "## 1   Supervised Classification\n",
    "A classifier is called supervised if it is built based on training corpora containing the correct label for each input. The framework used by supervised classification is shown in 1.1.\n",
    "\n",
    "## 1.1   Gender Identification\n",
    "In 4 we saw that male and female names have some distinctive characteristics. Names ending in a, e and i are likely to be female, while names ending in k, o, r, s and t are likely to be male. Let's build a classifier to model these differences more precisely.\n",
    "\n",
    "The first step in creating a classifier is deciding what features of the input are relevant, and how to encode those features. For this example, we'll start by just looking at the final letter of a given name. The following feature extractor function builds a dictionary containing relevant information about a given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last letter': 'k'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last letter': word[-1]}\n",
    "gender_features(\"Shrek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned dictionary, known as a feature set, maps from feature names to their values. \n",
    "\n",
    "Now that we've defined a feature extractor, we need to prepare a list of examples and corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alanna', 'female'), ('Schroeder', 'male'), ('Lizabeth', 'female'), ('Rinaldo', 'male'), ('Annette', 'female'), ('Wilmar', 'male'), ('Ilise', 'female'), ('Hiralal', 'male'), ('Hollyanne', 'female'), ('Sonni', 'female')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(labeled_names)\n",
    "print(labeled_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the feature extractor to process the names data, and divide the resulting list of feature sets into a training set and a test set. The training set is used to train a new \"naive Bayes\" classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with names that do not appear in classifier!\n",
    "classifier.classify(gender_features(\"Ninianano\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can examine the classifier to determine which features it found most effective for distinguishing the names' genders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last letter = 'a'            female : male   =     34.3 : 1.0\n",
      "             last letter = 'k'              male : female =     32.0 : 1.0\n",
      "             last letter = 'f'              male : female =     16.5 : 1.0\n",
      "             last letter = 'p'              male : female =     12.5 : 1.0\n",
      "             last letter = 'm'              male : female =     11.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This listing shows that the names in the training set that end in \"a\" are female 33 times more often than they are male, but names that end in \"k\" are male 32 times more often than they are female. These ratios are known as likelihood ratios, and can be useful for comparing different feature-outcome relationships.\n",
    "\n",
    "When working with large corpora, constructing a single list that contains the features of every instance can use up a large amount of memory. In these cases, use the function nltk.classify.apply_features, which returns an object that acts like a list but does not store all the feature sets in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import apply_features\n",
    "train_set = apply_features(gender_features, labeled_names[500:])\n",
    "test_set = apply_features(gender_features, labeled_names[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2   Choosing The Right Features\n",
    "Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method's ability to extract a good model. Much of the interesting work in building a classifier is deciding what features might be relevant, and how we can represent them. \n",
    "Typically, feature extractors are built through a process of trial-and-error, guided by intuitions about what information is relevant to the problem. It's common to start with a \"kitchen sink\" approach, including all the features that you can think of, and then checking to see which features actually are helpful.\n",
    "\n",
    "However, there are usually limits to the number of features that you should use with a given learning algorithm — if you provide too many features, then the algorithm will have a higher chance of relying on idiosyncrasies of your training data that don't generalize well to new examples. This problem is known as overfitting, and can be especially problematic when working with small training sets. For example, if we train a naive Bayes classifier using the feature extractor shown in 1.2, it will overfit the relatively small training set, resulting in a system whose accuracy is about 1% lower than the accuracy of a classifier that only pays attention to the final letter of each name.\n",
    "\n",
    "Once an initial set of features has been chosen, a very productive method for refining the feature set is error analysis. First, we select a development set, containing the corpus data for creating the model. This development set is then subdivided into the training set and the dev-test set.\n",
    "\n",
    "The training set is used to train the model, and the dev-test set is used to perform error analysis. The test set serves in our final evaluation of the system. For reasons discussed below, it is important that we employ a separate dev-test set for error analysis, rather than just using the test set. The division of the corpus data into different subsets is shown in http://www.nltk.org/images/corpus-org.png.\n",
    "Having divided the corpus into appropriate datasets, we train a model using the training set [1], and then run it on the dev-test set [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) #1\n",
    "print(nltk.classify.accuracy(classifier, devtest_set)) #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dev-test set, we can generate a list of the errors that the classifier makes when predicting name genders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then examine individual error cases where the model predicted the wrong label, and try to determine what additional pieces of information would allow it to make the right decision (or which existing pieces of information are tricking it into making the wrong decision). The feature set can then be adjusted accordingly. The names classifier that we have built generates about 100 errors on the dev-test corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female   guess=male     name=Abigail                       \n",
      "correct=female   guess=male     name=Adel                          \n",
      "correct=female   guess=male     name=Alis                          \n",
      "correct=female   guess=male     name=Alisun                        \n",
      "correct=female   guess=male     name=Allsun                        \n",
      "correct=female   guess=male     name=Allyn                         \n",
      "correct=female   guess=male     name=Amargo                        \n",
      "correct=female   guess=male     name=Ann                           \n",
      "correct=female   guess=male     name=Annabel                       \n",
      "correct=female   guess=male     name=Arabel                        \n",
      "correct=female   guess=male     name=Ardis                         \n",
      "correct=female   guess=male     name=Arlyn                         \n",
      "correct=female   guess=male     name=Beatriz                       \n",
      "correct=female   guess=male     name=Bev                           \n",
      "correct=female   guess=male     name=Birgit                        \n",
      "correct=female   guess=male     name=Blondell                      \n",
      "correct=female   guess=male     name=Brett                         \n",
      "correct=female   guess=male     name=Calypso                       \n",
      "correct=female   guess=male     name=Carolyn                       \n",
      "correct=female   guess=male     name=Carolynn                      \n",
      "correct=female   guess=male     name=Caron                         \n",
      "correct=female   guess=male     name=Caryn                         \n",
      "correct=female   guess=male     name=Cathryn                       \n",
      "correct=female   guess=male     name=Charil                        \n",
      "correct=female   guess=male     name=Charmian                      \n",
      "correct=female   guess=male     name=Charyl                        \n",
      "correct=female   guess=male     name=Cheryl                        \n",
      "correct=female   guess=male     name=Christabel                    \n",
      "correct=female   guess=male     name=Chrystel                      \n",
      "correct=female   guess=male     name=Claribel                      \n",
      "correct=female   guess=male     name=Clio                          \n",
      "correct=female   guess=male     name=Clovis                        \n",
      "correct=female   guess=male     name=Con                           \n",
      "correct=female   guess=male     name=Coral                         \n",
      "correct=female   guess=male     name=Coreen                        \n",
      "correct=female   guess=male     name=Crystal                       \n",
      "correct=female   guess=male     name=Deeann                        \n",
      "correct=female   guess=male     name=Demeter                       \n",
      "correct=female   guess=male     name=Demetris                      \n",
      "correct=female   guess=male     name=Denys                         \n",
      "correct=female   guess=male     name=Devon                         \n",
      "correct=female   guess=male     name=Dew                           \n",
      "correct=female   guess=male     name=Doralin                       \n",
      "correct=female   guess=male     name=Doro                          \n",
      "correct=female   guess=male     name=Eilis                         \n",
      "correct=female   guess=male     name=Em                            \n",
      "correct=female   guess=male     name=Erin                          \n",
      "correct=female   guess=male     name=Estel                         \n",
      "correct=female   guess=male     name=Estell                        \n",
      "correct=female   guess=male     name=Ethelin                       \n",
      "correct=female   guess=male     name=Farand                        \n",
      "correct=female   guess=male     name=Fern                          \n",
      "correct=female   guess=male     name=Fleur                         \n",
      "correct=female   guess=male     name=Flor                          \n",
      "correct=female   guess=male     name=Gail                          \n",
      "correct=female   guess=male     name=Gert                          \n",
      "correct=female   guess=male     name=Glennis                       \n",
      "correct=female   guess=male     name=Glynnis                       \n",
      "correct=female   guess=male     name=Gretel                        \n",
      "correct=female   guess=male     name=Guendolen                     \n",
      "correct=female   guess=male     name=Gus                           \n",
      "correct=female   guess=male     name=Gwenn                         \n",
      "correct=female   guess=male     name=Helyn                         \n",
      "correct=female   guess=male     name=Hester                        \n",
      "correct=female   guess=male     name=Ingaberg                      \n",
      "correct=female   guess=male     name=Isabeau                       \n",
      "correct=female   guess=male     name=Isador                        \n",
      "correct=female   guess=male     name=Ivett                         \n",
      "correct=female   guess=male     name=Janean                        \n",
      "correct=female   guess=male     name=Jaynell                       \n",
      "correct=female   guess=male     name=Jennifer                      \n",
      "correct=female   guess=male     name=Jerrilyn                      \n",
      "correct=female   guess=male     name=Jesselyn                      \n",
      "correct=female   guess=male     name=Jill                          \n",
      "correct=female   guess=male     name=Jilleen                       \n",
      "correct=female   guess=male     name=Joyous                        \n",
      "correct=female   guess=male     name=Kaitlyn                       \n",
      "correct=female   guess=male     name=Karalynn                      \n",
      "correct=female   guess=male     name=Karen                         \n",
      "correct=female   guess=male     name=Kaster                        \n",
      "correct=female   guess=male     name=Kellyann                      \n",
      "correct=female   guess=male     name=Kerrill                       \n",
      "correct=female   guess=male     name=Lillian                       \n",
      "correct=female   guess=male     name=Liz                           \n",
      "correct=female   guess=male     name=Lorilyn                       \n",
      "correct=female   guess=male     name=Madelin                       \n",
      "correct=female   guess=male     name=Madlin                        \n",
      "correct=female   guess=male     name=Margalo                       \n",
      "correct=female   guess=male     name=Marget                        \n",
      "correct=female   guess=male     name=Mariann                       \n",
      "correct=female   guess=male     name=Mariel                        \n",
      "correct=female   guess=male     name=Marigold                      \n",
      "correct=female   guess=male     name=Marillin                      \n",
      "correct=female   guess=male     name=Maris                         \n",
      "correct=female   guess=male     name=Meaghan                       \n",
      "correct=female   guess=male     name=Melisent                      \n",
      "correct=female   guess=male     name=Morgan                        \n",
      "correct=female   guess=male     name=Muffin                        \n",
      "correct=female   guess=male     name=Nanon                         \n",
      "correct=female   guess=male     name=Noel                          \n",
      "correct=female   guess=male     name=Noell                         \n",
      "correct=female   guess=male     name=Noelyn                        \n",
      "correct=female   guess=male     name=Norean                        \n",
      "correct=female   guess=male     name=Noreen                        \n",
      "correct=female   guess=male     name=Olwen                         \n",
      "correct=female   guess=male     name=Pru                           \n",
      "correct=female   guess=male     name=Rahal                         \n",
      "correct=female   guess=male     name=Renell                        \n",
      "correct=female   guess=male     name=Robin                         \n",
      "correct=female   guess=male     name=Robinet                       \n",
      "correct=female   guess=male     name=Rochell                       \n",
      "correct=female   guess=male     name=Rosaleen                      \n",
      "correct=female   guess=male     name=Rosalind                      \n",
      "correct=female   guess=male     name=Roseann                       \n",
      "correct=female   guess=male     name=Roselyn                       \n",
      "correct=female   guess=male     name=Row                           \n",
      "correct=female   guess=male     name=Sharron                       \n",
      "correct=female   guess=male     name=Sheila-Kathryn                \n",
      "correct=female   guess=male     name=Shell                         \n",
      "correct=female   guess=male     name=Sileas                        \n",
      "correct=female   guess=male     name=Sinead                        \n",
      "correct=female   guess=male     name=Starlin                       \n",
      "correct=female   guess=male     name=Tamiko                        \n",
      "correct=female   guess=male     name=Teriann                       \n",
      "correct=female   guess=male     name=Teryl                         \n",
      "correct=female   guess=male     name=Tomiko                        \n",
      "correct=female   guess=male     name=Viv                           \n",
      "correct=male     guess=female   name=Addie                         \n",
      "correct=male     guess=female   name=Aditya                        \n",
      "correct=male     guess=female   name=Ajai                          \n",
      "correct=male     guess=female   name=Ajay                          \n",
      "correct=male     guess=female   name=Alex                          \n",
      "correct=male     guess=female   name=Ambrosi                       \n",
      "correct=male     guess=female   name=Barclay                       \n",
      "correct=male     guess=female   name=Barnabe                       \n",
      "correct=male     guess=female   name=Barri                         \n",
      "correct=male     guess=female   name=Bela                          \n",
      "correct=male     guess=female   name=Benjy                         \n",
      "correct=male     guess=female   name=Berke                         \n",
      "correct=male     guess=female   name=Blare                         \n",
      "correct=male     guess=female   name=Bobby                         \n",
      "correct=male     guess=female   name=Bradly                        \n",
      "correct=male     guess=female   name=Bubba                         \n",
      "correct=male     guess=female   name=Burnaby                       \n",
      "correct=male     guess=female   name=Chancey                       \n",
      "correct=male     guess=female   name=Clancy                        \n",
      "correct=male     guess=female   name=Clarence                      \n",
      "correct=male     guess=female   name=Conway                        \n",
      "correct=male     guess=female   name=Courtney                      \n",
      "correct=male     guess=female   name=Curtice                       \n",
      "correct=male     guess=female   name=Dabney                        \n",
      "correct=male     guess=female   name=Dannie                        \n",
      "correct=male     guess=female   name=Dietrich                      \n",
      "correct=male     guess=female   name=Emmery                        \n",
      "correct=male     guess=female   name=Enrique                       \n",
      "correct=male     guess=female   name=Etienne                       \n",
      "correct=male     guess=female   name=Eustace                       \n",
      "correct=male     guess=female   name=Giorgi                        \n",
      "correct=male     guess=female   name=Hadley                        \n",
      "correct=male     guess=female   name=Helmuth                       \n",
      "correct=male     guess=female   name=Hillary                       \n",
      "correct=male     guess=female   name=Jay                           \n",
      "correct=male     guess=female   name=Jean-Pierre                   \n",
      "correct=male     guess=female   name=Jefferey                      \n",
      "correct=male     guess=female   name=Jeffrey                       \n",
      "correct=male     guess=female   name=Jeramie                       \n",
      "correct=male     guess=female   name=Jeremy                        \n",
      "correct=male     guess=female   name=Jerrome                       \n",
      "correct=male     guess=female   name=Jessie                        \n",
      "correct=male     guess=female   name=Jodi                          \n",
      "correct=male     guess=female   name=Jody                          \n",
      "correct=male     guess=female   name=Joey                          \n",
      "correct=male     guess=female   name=Jory                          \n",
      "correct=male     guess=female   name=Joshua                        \n",
      "correct=male     guess=female   name=Josiah                        \n",
      "correct=male     guess=female   name=Kalle                         \n",
      "correct=male     guess=female   name=Kelsey                        \n",
      "correct=male     guess=female   name=Kory                          \n",
      "correct=male     guess=female   name=Kyle                          \n",
      "correct=male     guess=female   name=Lazare                        \n",
      "correct=male     guess=female   name=Leigh                         \n",
      "correct=male     guess=female   name=Lemmie                        \n",
      "correct=male     guess=female   name=Mace                          \n",
      "correct=male     guess=female   name=Meade                         \n",
      "correct=male     guess=female   name=Michele                       \n",
      "correct=male     guess=female   name=Mickey                        \n",
      "correct=male     guess=female   name=Mika                          \n",
      "correct=male     guess=female   name=Mitch                         \n",
      "correct=male     guess=female   name=Moe                           \n",
      "correct=male     guess=female   name=Montgomery                    \n",
      "correct=male     guess=female   name=Morley                        \n",
      "correct=male     guess=female   name=Morly                         \n",
      "correct=male     guess=female   name=Mose                          \n",
      "correct=male     guess=female   name=Mugsy                         \n",
      "correct=male     guess=female   name=Munroe                        \n",
      "correct=male     guess=female   name=Murray                        \n",
      "correct=male     guess=female   name=Nichole                       \n",
      "correct=male     guess=female   name=Orbadiah                      \n",
      "correct=male     guess=female   name=Pascale                       \n",
      "correct=male     guess=female   name=Pearce                        \n",
      "correct=male     guess=female   name=Radcliffe                     \n",
      "correct=male     guess=female   name=Randi                         \n",
      "correct=male     guess=female   name=Randy                         \n",
      "correct=male     guess=female   name=Reece                         \n",
      "correct=male     guess=female   name=Reggy                         \n",
      "correct=male     guess=female   name=Rey                           \n",
      "correct=male     guess=female   name=Rice                          \n",
      "correct=male     guess=female   name=Rickey                        \n",
      "correct=male     guess=female   name=Roddie                        \n",
      "correct=male     guess=female   name=Rolfe                         \n",
      "correct=male     guess=female   name=Rolph                         \n",
      "correct=male     guess=female   name=Ruby                          \n",
      "correct=male     guess=female   name=Ruddie                        \n",
      "correct=male     guess=female   name=Rutledge                      \n",
      "correct=male     guess=female   name=Saundra                       \n",
      "correct=male     guess=female   name=Scarface                      \n",
      "correct=male     guess=female   name=Scotti                        \n",
      "correct=male     guess=female   name=Selby                         \n",
      "correct=male     guess=female   name=Shelby                        \n",
      "correct=male     guess=female   name=Si                            \n",
      "correct=male     guess=female   name=Skippie                       \n",
      "correct=male     guess=female   name=Stanley                       \n",
      "correct=male     guess=female   name=Stevie                        \n",
      "correct=male     guess=female   name=Stinky                        \n",
      "correct=male     guess=female   name=Sunny                         \n",
      "correct=male     guess=female   name=Temple                        \n",
      "correct=male     guess=female   name=Theodore                      \n",
      "correct=male     guess=female   name=Timmie                        \n",
      "correct=male     guess=female   name=Troy                          \n",
      "correct=male     guess=female   name=Tuckie                        \n",
      "correct=male     guess=female   name=Tucky                         \n",
      "correct=male     guess=female   name=Uli                           \n",
      "correct=male     guess=female   name=Welsh                         \n",
      "correct=male     guess=female   name=Westbrooke                    \n",
      "correct=male     guess=female   name=Wiley                         \n",
      "correct=male     guess=female   name=Worthy                        \n",
      "correct=male     guess=female   name=Yancey                        \n",
      "correct=male     guess=female   name=Yancy                         \n",
      "correct=male     guess=female   name=Zackariah                     \n",
      "correct=male     guess=female   name=Zolly                         \n"
     ]
    }
   ],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders. For example, names ending in yn appear to be predominantly female, despite the fact that names ending in n tend to be male; and names ending in ch are usually male, even though names that end in h tend to be female. We therefore adjust our feature extractor to include features for two-letter suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'suffix1': word[-1:],\n",
    "            'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error analysis procedure can then be repeated, checking for patterns in the errors that are made by the newly improved classifier. Each time the error analysis procedure is repeated, we should select a different dev-test/training split, to ensure that the classifier does not start to reflect idiosyncrasies in the dev-test set.\n",
    "\n",
    "But once we've used the dev-test set to help us develop the model, we can no longer trust that it will give us an accurate idea of how well the model would perform on new data. It is therefore important to keep the test set separate, and unused, until our model development is complete. At that point, we can use the test set to evaluate how well our model will perform on new input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4   Part-of-Speech Tagging\n",
    "In 5. we built a regular expression tagger that chooses a part-of-speech tag for a word by looking at the internal make-up of the word. However, this regular expression tagger had to be hand-crafted. Instead, we can train a classifier to work out which suffixes are most informative. Let's begin by finding out what the most common suffixes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "\n",
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a feature extractor function which checks a given word for these suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our feature extractor, we can use it to train a new \"decision tree\" classifier (to be discussed in 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f4debc2c3a06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# doesnt seem to work?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             tree = DecisionTreeClassifier.best_stump(\n\u001b[1;32m--> 154\u001b[1;33m                 feature_names, labeled_featuresets, verbose)\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             tree = DecisionTreeClassifier.best_binary_stump(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mbest_stump\u001b[1;34m(feature_names, labeled_featuresets, verbose)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mbest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_stump\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0mstump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m             \u001b[0mstump_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstump\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstump_error\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mstump\u001b[1;34m(feature_name, labeled_featuresets)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         label = FreqDist(label for (featureset, label)\n\u001b[1;32m--> 175\u001b[1;33m                          in labeled_featuresets).max()\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;31m# Find the best label for each value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         label = FreqDist(label for (featureset, label)\n\u001b[0m\u001b[0;32m    175\u001b[0m                          in labeled_featuresets).max()\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of decision tree models is that they are often fairly easy to interpret — we can even instruct NLTK to print them out as pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(e) == False: return 'IN'\n",
      "if endswith(e) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the classifier begins by checking whether a word ends with a comma — if so, then it will receive the special tag \",\". Next, the classifier checks if the word ends in \"the\", in which case it's almost certainly a determiner. This \"suffix\" gets used early by the decision tree because the word \"the\" is so common. Continuing on, the classifier checks if the word ends in \"s\". If so, then it's most likely to receive the verb tag VBZ (unless it's the word \"is\", which has a special tag BEZ), and if not, then it's most likely a noun (unless it's the punctuation mark \".\"). The actual classifier contains further nested if-then statements below the ones shown here, but the depth=4 argument just displays the top portion of the decision tree.\n",
    "\n",
    "## 1.5   Exploiting Context\n",
    "By augmenting the feature extraction function, we could modify this part-of-speech tagger to leverage a variety of other word-internal features, such as the length of the word, the number of syllables it contains, or its prefix. However, as long as the feature extractor just looks at the target word, we have no way to add features that depend on the context that the word appears in. But contextual features often provide powerful clues about the correct tag — for example, when tagging the word \"fly,\" knowing that the previous word is \"a\" will allow us to determine that it is functioning as a noun, not a verb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
