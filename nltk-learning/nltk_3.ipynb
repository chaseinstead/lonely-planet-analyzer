{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "\n",
    "with open('centralasia.txt', errors=\"ignore\") as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "with open('moe.txt', errors=\"ignore\") as file:\n",
    "    moe_txt = file.read()\n",
    "    \n",
    "file_in_words = word_tokenize(file_contents)\n",
    "text1 = Text(file_in_words)\n",
    "\n",
    "moe_words = word_tokenize(moe_txt)\n",
    "moe = Text(moe_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4   Generating Random Text with Bigrams\n",
    "We can use a conditional frequency distribution to create a table of bigrams (word pairs). (We introducted bigrams in 3.) The bigrams() function takes a list of words and builds a list of consecutive word pairs. Remember that, in order to see the result and not a cryptic \"generator object\", we need to use the list() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ausdrücken', 'ließe'), ('ließe', ';'), (';', 'wogegen'), ('wogegen', 'man'), ('man', 'bei'), ('bei', 'etwas'), ('etwas', 'so'), ('so', 'viel'), ('viel', 'Verwickelterem'), ('Verwickelterem', ','), (',', 'wie'), ('wie', 'es'), ('es', 'eine'), ('eine', 'Stadt'), ('Stadt', 'ist'), ('ist', ','), (',', 'in'), ('in', 'der'), ('der', 'man'), ('man', 'sich'), ('sich', 'aufhält'), ('aufhält', ','), (',', 'immer'), ('immer', 'durchaus'), ('durchaus', 'genau'), ('genau', 'wissen'), ('wissen', 'möchte'), ('möchte', ','), (',', 'welche'), ('welche', 'besondere'), ('besondere', 'Stadt'), ('Stadt', 'das'), ('das', 'sei'), ('sei', '.'), ('.', 'Es'), ('Es', 'lenkt'), ('lenkt', 'von'), ('von', 'Wichtigerem'), ('Wichtigerem', 'ab'), ('ab', '.'), ('.', 'Es'), ('Es', 'soll'), ('soll', 'also'), ('also', 'auf'), ('auf', 'den'), ('den', 'Namen'), ('Namen', 'der'), ('der', 'Stadt'), ('Stadt', 'kein'), ('kein', 'besonderer'), ('besonderer', 'Wert'), ('Wert', 'gelegt'), ('gelegt', 'werden'), ('werden', '.'), ('.', 'Wie'), ('Wie', 'alle'), ('alle', 'großen'), ('großen', 'Städte'), ('Städte', 'bestand'), ('bestand', 'sie'), ('sie', 'aus'), ('aus', 'Unregelmäßigkeit'), ('Unregelmäßigkeit', ','), (',', 'Wechsel'), ('Wechsel', ','), (',', 'Vorgleiten'), ('Vorgleiten', ','), (',', 'Nichtschritthalten'), ('Nichtschritthalten', ','), (',', 'Zusammenstößen'), ('Zusammenstößen', 'von'), ('von', 'Din¬gen'), ('Din¬gen', 'und'), ('und', 'Angelegenheiten'), ('Angelegenheiten', ','), (',', 'bodenlosen'), ('bodenlosen', 'Punkten'), ('Punkten', 'der'), ('der', 'Stille'), ('Stille', 'dazwischen'), ('dazwischen', ','), (',', 'aus'), ('aus', 'Bahnen'), ('Bahnen', 'und'), ('und', 'Ungebahntem'), ('Ungebahntem', ','), (',', 'aus'), ('aus', 'einem'), ('einem', 'großen'), ('großen', 'rhythmischen'), ('rhythmischen', 'Schlag'), ('Schlag', 'und'), ('und', 'der'), ('der', 'ewigen'), ('ewigen', 'Verstimmung'), ('Verstimmung', 'und'), ('und', 'Verschiebung'), ('Verschiebung', 'aller'), ('aller', 'Rhyth¬men'), ('Rhyth¬men', 'gegeneinander')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "a = list(nltk.bigrams(moe))\n",
    "print(a[400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2.2, we treat each word as a condition, and for each one we effectively create a frequency distribution over the following words. The function  generate_model() contains a simple loop to generate text. When we call the function, we choose a word (such as 'living') as our initial context, then once inside the loop, we print the current value of the variable word, and reset word to be the most likely token in that context (using max()); next time through the loop, we use that word as our new context. As you can see by inspecting the output, this simple approach to text generation tends to get stuck in loops; another method would be to randomly choose the next word from among the available words.\n",
    "\n",
    "Example 2.2 (code_random_text.py): Figure 2.2: Generating Random Text: this program obtains all bigrams from the text of the book of Genesis, then constructs a conditional frequency distribution to record which words are most likely to follow a given word; e.g., after the word living, the most likely word is creature; the generate_model() function uses this data, and a seed word, to generate random text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=\" \")\n",
    "        word = cfdist[word].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(mytext)\n",
    "cfd = ConditionalFreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'creature': 7, 'thing': 4, 'substance': 2, 'soul': 1, '.': 1, ',': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"living\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "living creature that he said , and the land of the land of the land "
     ]
    }
   ],
   "source": [
    "generate_model(cfd, \"living\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying the same with MoE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_text = moe\n",
    "bigramoe = nltk.bigrams(moe_text)\n",
    "cfd_moe = ConditionalFreqDist(bigramoe) # creates a conditional frequency distribution from a list of pairs (bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom an die sich in der Welt , daß sie sich in der Welt , "
     ]
    }
   ],
   "source": [
    "generate_model(cfd_moe, \"Atom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now for lonely planet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = text1\n",
    "bigramlp = nltk.bigrams(lp)\n",
    "cfd_lp = ConditionalFreqDist(bigramlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote Border Crossings Activities The best to the main road . The best to the "
     ]
    }
   ],
   "source": [
    "generate_model(cfd_lp, \"Remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laundries'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import language_processing as lp\n",
    "\n",
    "lp.plural(\"laundry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lp.vocab_wo_stopwords(\"english\", text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with wordnet\n",
    "Lemmas etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'railroad_train']\n",
      "['string', 'train']\n",
      "['caravan', 'train', 'wagon_train']\n",
      "['train']\n",
      "['train']\n",
      "['gearing', 'gear', 'geartrain', 'power_train', 'train']\n",
      "['train', 'develop', 'prepare', 'educate']\n",
      "['train', 'prepare']\n",
      "['discipline', 'train', 'check', 'condition']\n",
      "['prepare', 'groom', 'train']\n",
      "['educate', 'school', 'train', 'cultivate', 'civilize', 'civilise']\n",
      "['aim', 'take', 'train', 'take_aim', 'direct']\n",
      "['coach', 'train']\n",
      "['train']\n",
      "['train']\n",
      "['train', 'rail']\n",
      "['trail', 'train']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset in wn.synsets(\"train\"):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet makes it easy to navigate between concepts. For example, given a concept like motorcar, we can look at the concepts that are more specific; the (immediate) hyponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('boat_train.n.01'), Synset('car_train.n.01'), Synset('freight_train.n.01'), Synset('hospital_train.n.01'), Synset('mail_train.n.01'), Synset('passenger_train.n.01'), Synset('streamliner.n.01'), Synset('subway_train.n.01')]\n"
     ]
    }
   ],
   "source": [
    "train = wn.synset(\"train.n.01\")\n",
    "types_of_trains = train.hyponyms()\n",
    "print(types_of_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boat_train',\n",
       " 'car_train',\n",
       " 'freight_train',\n",
       " 'hospital_train',\n",
       " 'mail_train',\n",
       " 'passenger_train',\n",
       " 'rattler',\n",
       " 'streamliner',\n",
       " 'subway_train']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lemma.name() for synset in types_of_trains for lemma in synset.lemmas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also navigate up the hierarchy by visiting hypernyms. Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('public_transport.n.01')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'conveyance.n.03',\n",
       " 'public_transport.n.01',\n",
       " 'train.n.01']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = train.hypernym_paths()\n",
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the most general hypernyms (or root hypernyms) of a synset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important way to navigate the WordNet network is from items to their components (meronyms) or to the things they are contained in (holonyms). For example, the parts of a tree are its trunk, crown, and so on; the part_meronyms(). The substance a tree is made of includes heartwood and sapwood; the substance_meronyms(). A collection of trees forms a forest; the  member_holonyms()\n",
    "\n",
    "also: entailments(), antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measures have been defined over the collection of WordNet synsets which incorporate the above insight. For example, path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found). Comparing a synset with itself will return 1. Consider the following similarity scores, relating right whale to minke whale, orca, tortoise, and novel. Although the numbers won't mean much, they decrease as we move away from the semantic space of sea creatures to inanimate objects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
